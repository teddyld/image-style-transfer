{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSG-Net\n",
    "Zhang and Dana propose [MSG-Net](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Zhang_Multi-style_Generative_Network_for_Real-time_Transfer_ECCVW_2018_paper.pdf), an image style transfer architecture which aims to improve on the limitations in flexibility of standard generative methods by using a Generative Network with novel CoMatch Layer and Upsample Convolution.\n",
    "\n",
    "The MSG-Net is composed of a Siamese network which inputs the content and style images and shares weights with the encoder of the Transformer network. In the feed-forward pass, the CoMatch Layer embeds the style images with a 2D representation and learns to match the second-order feature statistics (Gram Matrix) of the style targets during training. This enables multi-style generation from a single feed-forward network. At the Transformation network's bottleneck, its architecture has been extended to use an Upsampling Convolution operation which improves image quality by removing checkerboard artifacts. This operation applies an integer stride convolution and outputs an upsampled featuremap. \n",
    "\n",
    "The loss network adopts the VGG architecture to minimise the mean-squared error of the content and style losses, reducing the perceptual difference between the input and output images of the generative network learning. \n",
    "\n",
    "| ![msgnet.png](./images/msgnet_architecture.png) | \n",
    "|:--:| \n",
    "| *MSG-Net Architecture* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils.data as data\n",
    "from utils.eval import compute_ssim, plot_results, calculate_fid_from_dataset, calc_content_loss, calc_style_loss, plot_training_history\n",
    "import models.MSGNet as MSGNet\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.CenterCrop(size=64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "content_trainloader, content_validloader, _, _, style_validloader, _ = data.get_dataloaders(bs=5, train_tf=train_tf, valid_tf=val_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model = MSGNet.Net(ngf=128)\n",
    "optimizer = optim.Adam(params=style_model.parameters(), lr=1e-3)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Load pretrained style model\n",
    "style_model.load_state_dict(torch.load('./models/output/MSG-Net/style_transfer_300_epochs.model'))\n",
    "\n",
    "# Load pretrained vgg16\n",
    "vgg = MSGNet.Vgg16()\n",
    "\n",
    "def init_vgg16(model_folder='./models/output/MSG-Net/'):\n",
    "    if not os.path.exists(os.path.join(model_folder, 'vgg16.weight')):\n",
    "        if not os.path.exists(os.path.join(model_folder, 'vgg16.pth')):\n",
    "            assert ValueError(\"Pretrained weights must be prepared, download vgg16.t7 weights and convert to vgg16.pth first\")\n",
    "        \n",
    "        pretrained = torch.load('./models/output/MSG-Net/vgg16.pth')\n",
    "        for (src, dst) in zip(list(pretrained.items()), vgg.parameters()):\n",
    "            dst.data[:] = src[1]\n",
    "        torch.save(vgg.state_dict(), os.path.join('./models/output/MSG-Net/', 'vgg16_msg.pth'))\n",
    "        \n",
    "init_vgg16()\n",
    "\n",
    "vgg.load_state_dict(torch.load('./models/output/MSG-Net/vgg16_msg.pth'))\n",
    "\n",
    "style_loader = data.StyleLoader()\n",
    "\n",
    "# Helper functions\n",
    "def preprocess_batch(batch):\n",
    "    batch = batch.transpose(0, 1)\n",
    "    (r, g, b) = torch.chunk(batch, 3)\n",
    "    batch = torch.cat((b, g, r))\n",
    "    batch = batch.transpose(0, 1)\n",
    "    return batch\n",
    "\n",
    "def subtract_imagenet_mean_batch(batch):\n",
    "    \"\"\"Subtract ImageNet mean pixel-wise from a BGR image.\"\"\"\n",
    "    tensortype = type(batch.data)\n",
    "    mean = tensortype(batch.data.size())\n",
    "    mean[:, 0, :, :] = 103.939\n",
    "    mean[:, 1, :, :] = 116.779\n",
    "    mean[:, 2, :, :] = 123.680\n",
    "    return batch - Variable(mean.to(device))\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.earlystopper import EarlyStopper\n",
    "\n",
    "def msgnet_train(style_model, optimizer, mse_loss, vgg, content_loader, style_loader, num_epochs=10, batch_size=5, save_model_dir='./models/output/MSG-Net/'):\n",
    "    style_model.to(device)\n",
    "    vgg.to(device)\n",
    "    early_stopper = EarlyStopper()\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    total_losses = []\n",
    "    for e in range(num_epochs):\n",
    "        style_model.train()\n",
    "        agg_content_loss = 0.\n",
    "        agg_style_loss = 0.\n",
    "        count = 0\n",
    "        print('.' * 64)\n",
    "        print(f\"--- Epoch {e + 1}/{num_epochs} ---\")\n",
    "        pbar = tqdm(content_loader, leave=False)\n",
    "        for batch_id, x in enumerate(pbar):\n",
    "            pbar.set_description(f\"Epoch [{e + 1}/{num_epochs}]\")\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "            x = Variable(preprocess_batch(x))\n",
    "            x = x.to(device)\n",
    "\n",
    "            style_v = style_loader.get(batch_id)\n",
    "            style_model.setTarget(style_v)\n",
    "        \n",
    "            style_v = subtract_imagenet_mean_batch(style_v)\n",
    "            features_style = vgg(style_v)\n",
    "            \n",
    "            gram_style = [gram_matrix(y) for y in features_style]\n",
    "\n",
    "            y = style_model(x)\n",
    "            xc = Variable(x.data.clone())\n",
    "\n",
    "            y = subtract_imagenet_mean_batch(y)\n",
    "            xc = subtract_imagenet_mean_batch(xc)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_xc = vgg(xc)\n",
    "\n",
    "            f_xc_c = Variable(features_xc[1].data, requires_grad=False)\n",
    "            content_loss = 1.0 * mse_loss(features_y[1], f_xc_c)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for m in range(len(features_y)):\n",
    "                gram_y = gram_matrix(features_y[m])\n",
    "                gram_s = Variable(gram_style[m].data, requires_grad=False).repeat(batch_size, 1, 1, 1)\n",
    "                style_loss += 5.0 * mse_loss(gram_y.unsqueeze(1), gram_s[:n_batch, :, :])\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            agg_content_loss += content_loss.item()\n",
    "            agg_style_loss += style_loss.item()\n",
    "            \n",
    "            if (batch_id + 1) % (4 * 500) == 0:\n",
    "                # Save model\n",
    "                style_model.eval()\n",
    "                style_model.cpu()\n",
    "                save_model_filename = \"Epoch_\" + str(e) + \"iters_\" + str(count) + \"_\" + \\\n",
    "                    str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "                    1.0) + \"_\" + str(5.0) + \".model\"\n",
    "                save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
    "                torch.save(style_model.state_dict(), save_model_path)\n",
    "                style_model.train()\n",
    "                style_model.cuda()\n",
    "                pbar.set_description(\"\\nCheckpoint, trained model saved at\", save_model_path)\n",
    "        \n",
    "        mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "            time.ctime(), e + 1, count, len(content_loader),\n",
    "            agg_content_loss / (batch_id + 1),\n",
    "            agg_style_loss / (batch_id + 1),\n",
    "            (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
    "        )\n",
    "        print(mesg)\n",
    "        \n",
    "        content_losses.append(agg_content_loss / (batch_id + 1))\n",
    "        style_losses.append(agg_style_loss / (batch_id + 1))\n",
    "        agg_total_loss = agg_content_loss + agg_style_loss\n",
    "        total_losses.append(agg_total_loss / (batch_id + 1))\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopper.early_stop(total_loss):\n",
    "            print(f'Stopping early at Epoch {e + 1}, min val loss failed to decrease after {early_stopper.get_patience()} epochs')\n",
    "            break\n",
    "    \n",
    "    # Save model\n",
    "    style_model.eval()\n",
    "    style_model.cpu()\n",
    "    save_model_filename = \"Final_epoch_\" + str(num_epochs) + \"_\" + \\\n",
    "        str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "        1.0) + \"_\" + str(5.0) + \".model\"\n",
    "    save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
    "    torch.save(style_model.state_dict(), save_model_path)\n",
    "\n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
    "    return content_losses, style_losses, total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_losses, style_losses, total_losses = msgnet_train(style_model, optimizer, mse_loss, vgg, content_trainloader, style_loader, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(content_losses, style_losses, total_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chkpt model for evaluation\n",
    "style_model = MSGNet.Net(ngf=128)\n",
    "model_dict = torch.load('./models/output/MSG-Net/style_transfer_300_epochs.model')\n",
    "model_dict_clone = model_dict.copy()\n",
    "for key, value in model_dict_clone.items():\n",
    "    if key.endswith(('running_mean', 'running_var')):\n",
    "        del model_dict[key]\n",
    "style_model.load_state_dict(model_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def tensor_to_img(tensor, cuda=False):\n",
    "    (b, g, r) = torch.chunk(tensor, 3)\n",
    "    tensor = torch.cat((r, g, b))\n",
    "    if cuda:\n",
    "        img = tensor.clone().cpu().clamp(0, 255).numpy()\n",
    "    else:\n",
    "        img = tensor.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype('uint8')\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def msgnet_eval(style_model, content_loader, style_loader):\n",
    "    style_model.to(device)\n",
    "    ssim_sum = 0.0\n",
    "    fid_sum = 0.0\n",
    "    running_content_loss, running_style_loss = 0.0, 0.0\n",
    "    total_samples = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for content, style in zip(content_loader, style_loader):\n",
    "            # Move content and style batches to device\n",
    "            content_images = content\n",
    "            content_images = content_images.to(device)\n",
    "            \n",
    "            style_images, style_labels = style\n",
    "            style_images, style_labels = style_images.to(device), style_labels.to(device)\n",
    "            \n",
    "            # Create stylised images\n",
    "            style_v = Variable(style_images)\n",
    "\n",
    "            content_images = Variable(content_images)\n",
    "            style_model.setTarget(style_v)\n",
    "            \n",
    "            output = style_model(content_images)\n",
    "            \n",
    "            # Compute quantitative evaluation metrics\n",
    "            ssim_sum += compute_ssim(content_images, output)\n",
    "            fid_sum += calculate_fid_from_dataset(content_images, output, device, dims=2048)\n",
    "            running_content_loss += calc_content_loss(content_images, output)\n",
    "            running_style_loss += calc_style_loss(style_images, output)\n",
    "            \n",
    "            # Convert output to PIL Image\n",
    "            stylised_images = []\n",
    "            for img in output:\n",
    "                img = tensor_to_img(img, cuda=True)\n",
    "                stylised_images.append(img)\n",
    "\n",
    "            # Display qualitative evaluation metrics on first batch\n",
    "            if count == 5:\n",
    "                plot_results(content_images, style_images, style_labels, stylised_images, nrows=5, model_name=\"MSG-Net\", msgnet=True)\n",
    "\n",
    "            total_samples += style_labels.size(0)\n",
    "            count += 1\n",
    "            \n",
    "    avg_ssim = ssim_sum / total_samples\n",
    "    avg_fid = fid_sum / total_samples\n",
    "    avg_content_loss = running_content_loss / total_samples\n",
    "    avg_style_loss = running_style_loss / total_samples\n",
    "    return avg_ssim, avg_fid, avg_content_loss, avg_style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgnet_ssim, msgnet_fid, msgnet_content_loss, msgnet_style_loss = msgnet_eval(style_model, content_validloader, style_validloader)\n",
    "print(\"--- MSG-Net results ---\")\n",
    "print(f\"Average SSIM = {msgnet_ssim:.7f}\")\n",
    "print(f\"Average FID = {msgnet_fid:.4f}\")\n",
    "print(f\"Average content loss = {msgnet_content_loss:.4f}\")\n",
    "print(f\"Average style loss = {msgnet_style_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
