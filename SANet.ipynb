{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import models.SANet as SANet\n",
    "\n",
    "COCO2014_DATA_PATH = './data/coco2014/'\n",
    "WIKIART_DATA_PATH = './data/wikiart/'\n",
    "STYLE_PATH = './data/style'\n",
    "\n",
    "WIKIART_STYLE_MAP = {\n",
    "    'Baroque': 1,\n",
    "    'Cubism': 2,\n",
    "    'Early_Renaissance': 3,\n",
    "    'Pointillism': 4,\n",
    "    'Ukiyo_e': 5,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO2014(Dataset):\n",
    "    def __init__(self, split, max_files, transform=None):\n",
    "        if split not in ['train', 'val', 'test']:\n",
    "            raise ValueError(f\"split must be 'train', 'val', or 'test'. Got: {split}\")\n",
    "        \n",
    "        split = split + '2014'\n",
    "        self.image_path = os.path.join(COCO2014_DATA_PATH, 'images', split)\n",
    "        images = os.listdir(self.image_path)\n",
    "        \n",
    "        if len(images) > max_files:\n",
    "            images = images[:max_files]\n",
    "        \n",
    "        self.images = images\n",
    "        self.length = len(images)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img = Image.open(os.path.join(self.image_path, img_name)).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, ttv, transform=None):\n",
    "        self.image_path = os.path.join(STYLE_PATH, ttv)\n",
    "        self.images = os.listdir(self.image_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img = Image.open(os.path.join(self.image_path, img_name)).convert('RGB')\n",
    "\n",
    "        img_style = [s for s in img_name.split('_') if s[0].isupper() or (s[0] == 'e' and len(s) == 1)]\n",
    "        label = WIKIART_STYLE_MAP[\"_\".join(img_style)]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "style_validset = StyleDataset('val', transform=val_tf)\n",
    "style_validloader = DataLoader(style_validset, 64, shuffle=False)\n",
    "content_validset = COCO2014('val', len(style_validset), val_tf)\n",
    "content_validloader = DataLoader(content_validset, 64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "def compute_ssim(content_images, stylised_images):\n",
    "    \"\"\"Returns the average structural similarity (SSIM) between the content and stylised images\"\"\"\n",
    "    ssim_sum = 0\n",
    "    for content, stylised in zip(content_images, stylised_images):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        content_img = content.detach().cpu().numpy()\n",
    "        stylised_img = stylised.detach().cpu().numpy()\n",
    "        \n",
    "        # Convert image to grayscale\n",
    "        gray_content = content_img[0, :, :]\n",
    "        gray_stylised = stylised_img[0, :, :]\n",
    "        \n",
    "        # Compute SSIM\n",
    "        score = structural_similarity(gray_content, gray_stylised, data_range=1.0)\n",
    "        ssim_sum += score\n",
    "    \n",
    "    return ssim_sum\n",
    "\n",
    "# Code below has been adapted from https://github.com/EndyWon/MicroAST/blob/main/metrics/calc_cs_loss.py\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def calc_content_loss(input, target):\n",
    "    assert (input.size() == target.size())\n",
    "    return torch.nn.MSELoss()(input, target)\n",
    "\n",
    "def calc_style_loss(input, target):\n",
    "    input_mean, input_std = calc_mean_std(input)\n",
    "    target_mean, target_std = calc_mean_std(target)\n",
    "    return torch.nn.MSELoss()(input_mean, target_mean) + torch.nn.MSELoss()(input_std, target_std)\n",
    "\n",
    "def compute_style_loss(style_images, stylised_images, net):\n",
    "    \"\"\"Compute the style loss between the style images and stylised images\"\"\"\n",
    "    enc_1 = net[0]\n",
    "    enc_2 = net[1]\n",
    "    enc_3 = net[2]\n",
    "    enc_4 = net[3]\n",
    "    enc_5 = net[4]\n",
    "    loss_s = 0.0\n",
    "    output1_1 = enc_1(style_images)\n",
    "    style1_1 = enc_1(stylised_images) \n",
    "    loss_s += calc_style_loss(output1_1, style1_1)\n",
    "    \n",
    "    output2_1 = enc_2(output1_1)\n",
    "    style2_1 = enc_2(style1_1)\n",
    "    loss_s += calc_style_loss(output2_1, style2_1)\n",
    "\n",
    "    output3_1 = enc_3(output2_1)\n",
    "    style3_1 = enc_3(style2_1)\n",
    "    loss_s += calc_style_loss(output3_1, style3_1)\n",
    "\n",
    "    output4_1 = enc_4(output3_1)\n",
    "    style4_1 = enc_4(style3_1)\n",
    "    loss_s += calc_style_loss(output4_1, style4_1)\n",
    "\n",
    "    output5_1 = enc_5(output4_1)\n",
    "    style5_1 = enc_5(style4_1)\n",
    "    loss_s += calc_style_loss(output5_1, style5_1)\n",
    "    \n",
    "    return float(loss_s / 5)\n",
    "        \n",
    "def compute_content_loss(content_images, stylised_images, net):\n",
    "    \"\"\"Compute the content loss between the content images and stylised images\"\"\"\n",
    "    enc_1 = net[0]\n",
    "    enc_2 = net[1]\n",
    "    enc_3 = net[2]\n",
    "    enc_4 = net[3]\n",
    "    enc_5 = net[4]\n",
    "    \n",
    "    loss_c = 0.0\n",
    "\n",
    "    output1 = enc_4(enc_3(enc_2(enc_1(content_images))))\n",
    "    content1 = enc_4(enc_3(enc_2(enc_1(stylised_images))))\n",
    "\n",
    "    loss_c += calc_content_loss(output1, content1)\n",
    "    \n",
    "    output2 = enc_5(output1)\n",
    "    content2 = enc_5(content1)\n",
    "    \n",
    "    loss_c += calc_content_loss(output2, content2)\n",
    "        \n",
    "    return float(loss_c / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "\n",
    "def plot_results(content_images, style_images, stylised_images, nrows=5, ncols=5):\n",
    "    \"\"\"Plot the stylisation results\"\"\"\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    for i in range(nrows):\n",
    "        content_img = content_images[i].cpu()\n",
    "        style_img = style_images[i].cpu()\n",
    "        stylised_img = stylised_images[i].cpu()\n",
    "        \n",
    "        # Plot content, style and stylised images\n",
    "        axes[i][0].imshow(content_img.permute(1, 2, 0))\n",
    "        axes[i][1].imshow(style_img.permute(1, 2, 0))\n",
    "        axes[i][2].imshow(stylised_img.permute(1, 2, 0))\n",
    "        \n",
    "        # Plot canny edges of content and stylised image\n",
    "        content_img = content_img.permute(1, 2, 0).numpy()[:, :, 0]\n",
    "        stylised_img = stylised_img.permute(1, 2, 0).numpy()[:, :, 0]\n",
    "        axes[i][3].imshow(feature.canny(content_img, sigma=1), cmap=\"cividis\")\n",
    "        axes[i][4].imshow(feature.canny(stylised_img, sigma=1), cmap=\"cividis\")\n",
    "        \n",
    "        # Set subplot titles\n",
    "        if i == nrows - 1:\n",
    "            axes[i][0].text(0.5, -0.05, \"Content\", size=12, ha=\"center\", transform=axes[i][0].transAxes)\n",
    "            axes[i][1].text(0.5, -0.05, \"Style\", size=12, ha=\"center\", transform=axes[i][1].transAxes)\n",
    "            axes[i][2].text(0.5, -0.05, \"SANet\", size=12, ha=\"center\", transform=axes[i][2].transAxes)\n",
    "            axes[i][3].text(0.5, -0.05, \"Content Canny Edges\", size=12, ha=\"center\", transform=axes[i][3].transAxes)\n",
    "            axes[i][4].text(0.5, -0.05, \"SANet Canny Edges\", size=12, ha=\"center\", transform=axes[i][4].transAxes)\n",
    "\n",
    "    fig.suptitle('SANet Results', size=21, y=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = SANet.decoder\n",
    "vgg = SANet.vgg\n",
    "transform = SANet.Transform(in_planes=512)\n",
    "decoder.load_state_dict(torch.load('./models/output/decoder_iter_500000.pth'))\n",
    "vgg.load_state_dict(torch.load('./models/output/vgg_normalised.pth'))\n",
    "transform.load_state_dict(torch.load('./models/output/transformer_iter_500000.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SANet_eval(decoder, vgg, transform, content_loader, style_loader):\n",
    "    decoder.eval()\n",
    "    transform.eval()\n",
    "    vgg.eval()\n",
    "\n",
    "    enc_1 = nn.Sequential(*list(vgg.children())[:4])     # input -> relu1_1\n",
    "    enc_2 = nn.Sequential(*list(vgg.children())[4:11])   # relu1_1 -> relu2_1\n",
    "    enc_3 = nn.Sequential(*list(vgg.children())[11:18])  # relu2_1 -> relu3_1\n",
    "    enc_4 = nn.Sequential(*list(vgg.children())[18:31])  # relu3_1 -> relu4_1\n",
    "    enc_5 = nn.Sequential(*list(vgg.children())[31:44])  # relu4_1 -> relu5_1\n",
    "\n",
    "    enc_1.to(device)\n",
    "    enc_2.to(device)\n",
    "    enc_3.to(device)\n",
    "    enc_4.to(device)\n",
    "    enc_5.to(device)\n",
    "    transform.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ssim_sum = 0.0\n",
    "        total_samples = 0\n",
    "        running_content_loss, running_style_loss = 0.0, 0.0\n",
    "        for content, style in zip(content_loader, style_loader):\n",
    "            # Move content and style batches to device\n",
    "            content_images = content\n",
    "            content_images = content_images.to(device)\n",
    "            \n",
    "            style_images, style_labels = style\n",
    "            style_images, style_labels = style_images.to(device), style_labels.to(device)\n",
    "            \n",
    "            # Create stylised images\n",
    "            content4_1 = enc_4(enc_3(enc_2(enc_1(content_images))))\n",
    "            content5_1 = enc_5(content4_1)\n",
    "\n",
    "            style4_1 = enc_4(enc_3(enc_2(enc_1(style_images))))\n",
    "            style5_1 = enc_5(style4_1)\n",
    "\n",
    "            stylised_images = decoder(transform(content4_1, style4_1, content5_1, style5_1))\n",
    "            stylised_images.clamp(0, 255)\n",
    "\n",
    "            # Display qualitative evaluation metrics on first batch\n",
    "            if total_samples == 0:\n",
    "                plot_results(content_images, style_images, stylised_images)\n",
    "\n",
    "            # Compute quantitative evaluation metrics\n",
    "            ssim_sum += compute_ssim(content_images, stylised_images)\n",
    "            running_content_loss += compute_content_loss(content_images, stylised_images, [enc_1, enc_2, enc_3, enc_4, enc_5])\n",
    "            running_style_loss += compute_style_loss(style_images, stylised_images, [enc_1, enc_2, enc_3, enc_4, enc_5])\n",
    "            \n",
    "            total_samples += style_labels.size(0)\n",
    "            \n",
    "    print(f\"Average SSIM = {ssim_sum / total_samples:.4f}\")\n",
    "    print(f\"Average content loss = {running_content_loss / total_samples:.4f}\")\n",
    "    print(f\"Average style loss = {running_style_loss / total_samples:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SANet_eval(decoder, vgg, transform, content_validloader, style_validloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
