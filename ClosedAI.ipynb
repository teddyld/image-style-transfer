{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group name: ClosedAI\n",
    "Project ID: 043\n",
    "\n",
    "Project: Images Style Transfer\n",
    "\n",
    "\n",
    "| Name    | Email | Student ID |\n",
    "| -------- | ------- | ------- |\n",
    "| Elijah Maron  | z5372352@ad.unsw.edu.au   | z5372352\n",
    "| Hari Birudavolu | z5419889@ad.unsw.edu.au     | z5419889\n",
    "| Michael Girikallo    | z5416925@ad.unsw.edu.au   | z5416925\n",
    "| Tianshuo Xu    | z5358205@ad.unsw.edu.au   | z5358205\n",
    "| Vincent Pham    | z5363266@ad.unsw.edu.au   | z5363266\n",
    "\n",
    "Codebase available on [GitHub](https://github.com/teddyld/image-style-transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Motivation](#motivation)\n",
    "3. [Problem Statement](#problem-statement)\n",
    "4. [Data Sources](#data-sources)\n",
    "5. [Exploratory Analysis of Data](#exploratory-analysis)\n",
    "6. [Models and Methods](#models-methods)\n",
    "7. [Results](#results)\n",
    "8. [Discussion](#discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"introduction\">Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id=\"motivation\">Motivation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id=\"problem-statement\">Problem Statement</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id=\"data-sources\">Data Sources</a>\n",
    "\n",
    "This section first explains the process to prepare the datasets and describes the characteristics of the COCO2014 and WikiArt dataset used for Image Style Transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to data sources in disk\n",
    "COCO2014_DATA_PATH = './data/coco2014/'\n",
    "WIKIART_DATA_PATH = './data/wikiart/'\n",
    "STYLE_PATH = './data/style'\n",
    "\n",
    "# Maps the name of the style classes to integers\n",
    "WIKIART_STYLE_MAP = {\n",
    "    'Baroque': 1,\n",
    "    'Cubism': 2,\n",
    "    'Early_Renaissance': 3,\n",
    "    'Pointillism': 4,\n",
    "    'Ukiyo_e': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Data Preparation\n",
    "\n",
    "After downloading our datasets from Kaggle, we prepared the style dataset from WikiArt by splitting the dataset into train, validation, and test splits. Note that this style dataset is a subset of the WikiArt dataset on Kaggle as it only contains five style classes: Baroque, Cubism, Early Renaissance, Pointillism, and Ukiyoe. We justify this choice later in [this section](#style-justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initial Data structure\n",
    "/data\n",
    "    /coco2014\n",
    "        /annotations\n",
    "        /images\n",
    "            /test2014\n",
    "            /train2014\n",
    "            /val2014\n",
    "        /labels\n",
    "    /wikiart\n",
    "        /Abstract_Expressionism\n",
    "            /aaron-siskind_acolman-1-1955.jpg\n",
    "            ...\n",
    "        /Action_painting\n",
    "        ...\n",
    "        /Ukiyo_e\n",
    "        /classes.csv\n",
    "        /wclasses.csv\n",
    "        \n",
    "Target Data structure\n",
    "/data\n",
    "    /coco2014\n",
    "        /annotations\n",
    "        /images\n",
    "            /test2014\n",
    "            /train2014\n",
    "            /val2014\n",
    "        /labels\n",
    "    /style\n",
    "        /test\n",
    "        /train\n",
    "        /val\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Flattening the WikiArt directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(include=['Cubism', 'Baroque', 'Early_Renaissance', 'Pointillism', 'Ukiyo_e'], src=WIKIART_DATA_PATH):\n",
    "    \"\"\"\n",
    "    Given a directory src, unpack the contents of its sub-directories into the directory src. Only the names of the sub-directories in the 'include' list are unpacked\n",
    "    \"\"\"\n",
    "    subdirs = [dir for dir in os.listdir(src) if os.path.isdir(os.path.join(src, dir)) and dir in include]\n",
    "\n",
    "    for dir in subdirs:\n",
    "        subdir_path = os.path.join(src, dir)\n",
    "        files = os.listdir(subdir_path)\n",
    "        loop = tqdm(files)\n",
    "        loop.set_description(f'Unpacking sub-directory {dir}')\n",
    "        for file in loop:\n",
    "            # Prepend the style class the file is from\n",
    "            os.rename(os.path.join(subdir_path, file), os.path.join(src, dir + '_' + file))\n",
    "            \n",
    "        os.rmdir(subdir_path)\n",
    "    \n",
    "flatten_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Splitting the WikiArt dataset into train, validation, and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(src, dest, files, ttv):\n",
    "    \"\"\"\n",
    "    Write files from src to dest\n",
    "    \"\"\"\n",
    "    loop = tqdm(files, total=len(files))\n",
    "    loop.set_description(f\"Writing {ttv} split\")\n",
    "    for file in loop:\n",
    "        src_path = os.path.join(src, file)\n",
    "        dest_path = os.path.join(dest, file)\n",
    "        shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "def split_data(src, dest, split_size=0.8, max_files=30000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Given a directory src, create train, val, and test split directories in directory dest\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    if not (0 < split_size < 1):\n",
    "        raise ValueError(f\"split_size must be between 0 and 1. Got: {split_size}\")\n",
    "    \n",
    "    all_files = [file for file in os.listdir(src) if file.endswith('.jpg')]\n",
    "    \n",
    "    if not all_files:\n",
    "        raise ValueError(f\"src directory did not contain any files\")\n",
    "    \n",
    "    \n",
    "    if len(all_files) > max_files:\n",
    "        all_files = all_files[:max_files]\n",
    "        \n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    # Create train split\n",
    "    train_files, remaining_files = train_test_split(all_files, train_size=split_size, random_state=random_seed)\n",
    "    \n",
    "    # Create validation and test split\n",
    "    validation_files, test_files = train_test_split(remaining_files, test_size=0.5, random_state=random_seed)\n",
    "    \n",
    "    # Make destination directories\n",
    "    split = ['train', 'val', 'test']\n",
    "    \n",
    "    for ttv in split:\n",
    "        split_path = os.path.join(dest, ttv)\n",
    "        if os.path.exists(split_path):\n",
    "            shutil.rmtree(split_path)\n",
    "        \n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        \n",
    "    make_split(src, os.path.join(dest, 'train'), train_files, 'train')\n",
    "    make_split(src, os.path.join(dest, 'val'), validation_files, 'val')\n",
    "    make_split(src, os.path.join(dest, 'test'), test_files, 'test')\n",
    "\n",
    "split_data(WIKIART_DATA_PATH, STYLE_PATH, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. <a id=\"style-justification\">Data Source Characteristics</a>\n",
    "\n",
    "With our data sources prepared, we now describe and [illustrate](#dataset-examples) the characteristics of our content and style datasets. For our content images, we use the COCO2014 dataset whilst for our style images we use the WikiArt dataset.\n",
    "\n",
    "The COCO 2014 Dataset on [Kaggle](https://www.kaggle.com/datasets/jeffaudi/coco-2014-dataset-for-yolov3/) is a popular dataset developed by [Lin et al.](https://arxiv.org/pdf/1405.0312) with 80 classes, 82,783 training and 40,504 validation images in RGB format. During image collection, the authors filtered out iconic images in favour of non-iconic images. Iconic images are characterised by single large objects in a canonical perspective centered in the image. Evidently, research from [Torralba and Efros](https://ieeexplore-ieee-org.wwwproxy1.library.unsw.edu.au/document/5995347) indicates that a lack of contextual information and non-canonical viewpoints in iconic images may lead to decreased generalization of datasets from capture bias and negative set bias. Therefore, by removing iconic images, the COCO dataset is well-generalised and provides rich contextual relationships between objects in their natural environments. For the task of image style transfer, the COCO 2014 dataset is an exceptional source for 'content' images. Chiefly, the prevalance of non-iconic images provide diverse structural features which challenge the robustness and efficacy of style transfer model performance. For example, models like AdaAttn and MAST hyper-fixate on local structure leading to style leakage [(Xu et al.)](https://arxiv.org/abs/2304.00414). Contributing to the diverse structural features is the breadth of object classes collected for this dataset which will challenge a model's ability to generalize.\n",
    "\n",
    "The WikiArt Dataset accessed from [Kaggle](https://www.kaggle.com/datasets/steubk/wikiart) is dataset of 80,020 images from 1119 different artists with 27 distinct styles classes. The images are sourced from WikiArt.org, an encylopedia of art. Combined, the 27 styles classes offer a wide range of unique salient features ranging from the small visible brushstrokes of 'Impressionism', the vibrant and bold colouring of 'Fauvism' to visually blending of small dots of colour which define 'Pointillism'. For the task of image style transfer, the WikiArt dataset is a standard benchmark for 'style' images used across research. The large number of classes alongside the depth of salient features makes WikiArt useful in evaluating the robustness and reliability of transferring artistic styles while preserving style patterns. The quantity of style classes will negatively affect model performance as well as dramatically increase the cost of computation. Hence, we reduce the number of style classes we consider to five: Cubism, Pointillism, Baroque, Early_Renaissance, and Ukiyo_e. We have selected these styles based on the criteria of uniqueness and within-class consistency of style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content dataset\n",
    "class COCO2014(Dataset):\n",
    "    def __init__(self, split, max_files, transform=None):\n",
    "        if split not in ['train', 'val', 'test']:\n",
    "            raise ValueError(f\"split must be 'train', 'val', or 'test'. Got: {split}\")\n",
    "        \n",
    "        split = split + '2014'\n",
    "        self.image_path = os.path.join(COCO2014_DATA_PATH, 'images', split)\n",
    "        images = os.listdir(self.image_path)\n",
    "        \n",
    "        if len(images) > max_files:\n",
    "            images = images[:max_files]\n",
    "        \n",
    "        self.images = images\n",
    "        self.length = len(images)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img = Image.open(os.path.join(self.image_path, img_name)).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img # Note that we do not need the label of the content classes\n",
    "\n",
    "# Style dataset\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, ttv, transform=None):\n",
    "        self.image_path = os.path.join(STYLE_PATH, ttv)\n",
    "        self.images = os.listdir(self.image_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img = Image.open(os.path.join(self.image_path, img_name)).convert('RGB')\n",
    "\n",
    "        # Get image label from file name\n",
    "        img_style = [s for s in img_name.split('_') if s[0].isupper() or (s[0] == 'e' and len(s) == 1)]\n",
    "        label = WIKIART_STYLE_MAP[\"_\".join(img_style)]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For demonstration we use the Resize transform and plot images from the training split\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "content_trainset = COCO2014('train', 24000, train_tf)\n",
    "style_trainset = StyleDataset('train', transform=train_tf)\n",
    "\n",
    "content_trainloader = DataLoader(content_trainset, 64, shuffle=True)\n",
    "style_trainloader = DataLoader(style_trainset, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) <a id=\"dataset-examples\">Examples from our Datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(loader, title, styleset=False):\n",
    "    \"\"\"\n",
    "    Plot images from a batch of a DataLoader\n",
    "    Args:\n",
    "        loader (iterable) - a Pytorch DataLoader class\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(10, 5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    for batch in loader:\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if styleset:\n",
    "                images, labels = batch\n",
    "                label = list(WIKIART_STYLE_MAP.keys())[list(WIKIART_STYLE_MAP.values()).index(labels[i])]\n",
    "                ax.set_title(label)\n",
    "            else:\n",
    "                images = batch\n",
    "            image = images[i].permute(1, 2, 0)\n",
    "            ax.imshow(image)\n",
    "        break\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_dataset(content_trainloader, 'COCO2014 Data')\n",
    "plot_dataset(style_trainloader, 'WikiArt Data', styleset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id=\"exploratory-analysis\">Exploratory Analysis of Data</a>\n",
    "\n",
    "In our exploration of the data, we investigate the efficacy of evaluating the structural features of images using Canny and Sobel edge detection, illustrate the class distribution of style classes, and describe the characteristics and properties of each of our style classes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Structural features of images using Canny and Sobel edge detection\n",
    "\n",
    "Canny edge detection is an edge detection technique that extracts structural information from images. We utilise the function `skimage.feature.canny()` implemented in the scikit-image library to qualitatively display the structural features of images. The function uses the following steps:\n",
    "\n",
    "1. A Gaussian blur with standard deviation of the Gaussian kernel equal to `sigma` to reduce image noise\n",
    "2. Sobel edge detection\n",
    "3. Apply non-maximum suppression to remove pixels far from edges \n",
    "4. Hysteresis thresholding is applied, labelling all points above the high threshold value as edges and recursively labelling any point above the low threshold value connected to a labeled point as an edge.\n",
    "\n",
    "Sobel edge detection is a convolution-based method used for image edge detection that approximates the gradient of the image intensity. By convolving with the 3x3 Sobel kernel, the algorithm estimates the gradient magnitude and direction between regions of low and high intensity, hence, emphasising the edges of objects.\n",
    "\n",
    "By comparing edges of the image output of the style transfer with its content image source, we qualitatively evaluate the change in structural features. Indicators of performant style transfer are consistent global structure between the output and content input images by examining object alignment and shape. Furthermore, small local details in the content image should not be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import canny\n",
    "from skimage.filters import sobel\n",
    "\n",
    "# Get random image from content dataset\n",
    "images = next(iter(content_trainloader))\n",
    "random_index = random.randint(0, len(images) - 1)\n",
    "img = images[random_index]\n",
    "\n",
    "def plot_canny_edges(img, sigma=1):\n",
    "  \"\"\"\n",
    "  Apply canny edge detection to the image with 'sigma' parameter\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
    "  \n",
    "  # Change view to (height, width, channels)\n",
    "  img = img.permute(1, 2, 0)\n",
    "  ax[0].imshow(img)\n",
    "  \n",
    "  # Convert tensor to numpy and single channel\n",
    "  img = img.numpy()[:, :, 0]\n",
    "  ax[1].imshow(canny(img, sigma=sigma), cmap=\"copper\")\n",
    "  \n",
    "  ax[2].imshow(sobel(img), cmap=\"copper\")\n",
    "\n",
    "plot_canny_edges(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. The characteristics and properties of styles\n",
    "\n",
    "In this section, we explain the characteristics and properties of the five style classes selected as well as illustrate these images by plotting them without alteration from the original source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_style(style):\n",
    "    \"\"\"\n",
    "    Plot four random images with the provided style in the train directory\n",
    "    \"\"\"\n",
    "    style_images = [img for img in os.listdir(os.path.join(STYLE_PATH, 'train')) if img.startswith(style)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    fig.suptitle(f\"Style - {style}\", y=0.8)\n",
    "    for i, img in enumerate(style_images[:4]):\n",
    "        axes[i].imshow(Image.open(os.path.join(STYLE_PATH, 'train', img)).convert('RGB'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Cubism\n",
    "\n",
    "Cubism, popularised by artists such as Pablo Picasso and Georges Braque in the 20th century is a style of art characterised by its use of interweaving planes and lines to depict abstract objects. Notably, Cubism art lacks form, and often, the arrangement of simple planes merges the foreground and background. Artworks in Cubism use muted tones of blacks and grays or use bright and solid colours. \n",
    "\n",
    "The Cubism class contains 2235 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style('Cubism')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Pointillism\n",
    "\n",
    "Artworks in the style of Pointillism are defined by their distinct painting technique, utilising small dots of colour so that from a distance, they visually blend together to form a vibrant composition. \n",
    "\n",
    "The Pointillism class contains 513 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style('Pointillism')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Baroque\n",
    "\n",
    "The Baroque style refers to artworks derived from Europe from the early 17th to mid-18th century. Artworks of this style are associated with deep colors, dramatic light, sharp shadows, and dark backgrounds. \n",
    "\n",
    "The Baroque class contains 4240 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style('Baroque')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) Early Renaissance\n",
    "\n",
    "Artworks in the Early Renaissance period are characterised by the realistic depiction of human anatomy and space from mythology and religion. The paintings of this period use muted colours whilst sculptures of human forms are made of bronze and marble. \n",
    "\n",
    "The Early Renaissance class contains 1391 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style('Early_Renaissance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v) Ukiyo-e\n",
    "\n",
    "Ukiyo-e refers to a style of Japanese paintings and woodblock prints from the Edo period. The style is defined by its bold and flat brush strokes, asymmetric composition, and unusual graphical perspective. In stark contrast, the artwork is complimented by its colourfulness in depictions of flora and fauna. Furthermore, artworks of this style often contain text, written with black or red ink. The text stands prominently against the style's generous use of negative space as the artwork's background is often only painted with a single colour.\n",
    "\n",
    "The Ukiyo-e class contains 1167 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style('Ukiyo_e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Train style class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "train_all_files = os.listdir(os.path.join('./data/style/train'))\n",
    "\n",
    "style_frequency = {\n",
    "    \"Baroque\": 0,\n",
    "    \"Cubism\": 0,\n",
    "    \"Early_Renaissance\": 0,\n",
    "    \"Pointillism\": 0,\n",
    "    \"Ukiyo_e\": 0,\n",
    "}\n",
    "\n",
    "for img_name in train_all_files:\n",
    "\n",
    "    img_style = [s for s in img_name.split('_') if s[0].isupper() or (s[0] == 'e' and len(s) == 1)]\n",
    "    img_style = \"_\".join(img_style)\n",
    "    \n",
    "    style_frequency[img_style] += 1\n",
    "\n",
    "keys = list(style_frequency.keys())\n",
    "vals = [style_frequency[k] for k in keys]\n",
    "\n",
    "plot = sns.barplot(style_frequency, x=keys, y=vals, hue=keys)\n",
    "plot.set_ylabel(\"Count\")\n",
    "plot.set_xlabel(\"Style Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id=\"models-methods\">Models and Methods</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Choice of Models\n",
    "\n",
    "We trained two generative networks: MSG-Net and CycleGAN and evaluated two pretrained traditional CNN methods: AdaIN and SANet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# Models\n",
    "import models.AdaIN as adain\n",
    "import models.SANet as SANet\n",
    "import models.CycleGAN as cyclegan \n",
    "import models.MSGNet as MSGNet\n",
    "\n",
    "# Utils\n",
    "import utils.data as data\n",
    "from utils.eval import compute_ssim, plot_results, calc_content_loss, calc_style_loss, calculate_fid_from_dataset, plot_training_history\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Traditional CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) AdaIN\n",
    "  \n",
    "Huang and Belongie propose [AdaIN](https://arxiv.org/abs/1703.06868), an image style transfer architecture that aims to improve performance compared to previous works by using an adaptive instance normalization mechanism.  \n",
    "\n",
    "Initially, the content image and the style image are passed through a pretrained VGG encoder to extract their feature maps, this will give out content feature map and style feature map.   \n",
    " \n",
    "#### VGG  \n",
    "\n",
    "| ![vgg_architecture.png](./images/vgg_architecture.png) | \n",
    "|:--:| \n",
    "| *VGG Architecture* |\n",
    "\n",
    "Visual Geometry Group (VGG) is network is a deep convolutional neural network architecture. It primarily improves image classification and feature extraction performance by using smaller convolution kernels and a deeper network structure. The fundamental building blocks of the VGG network are 3x3 convolutional kernels and 2x2 max pooling layers, small convolutional kernels can captures fine features, and stacking multiple convolutional layers mimics a larger receptive field. VGG enhances the expressiveness and accuracy of the model by increasing the network depth (more convolution layers). Deeper networks can learn more complex and advanced features. Downsampling is performed using a fixed-size 2x2 max pooling layer to gradually reduce the spatial dimension of the feature map while increasing the receptive field. This design makes it excellent in feature extraction, such as style transfer, image retrieval, etc.  \n",
    "\n",
    "The extracted feature maps enter the AdaIN module. In this module, the mean and standard deviation of the content feature map are adjusted to match the mean and standard deviation of the style feature map, thus aligning the statistical properties of the content feature map with those of the style feature map. The adjusted feature map then passed through the decoder to generate an image with the target style. Finally, the generated image is passed through the VGG encoder again to extract features for calculating the loss.\n",
    "  \n",
    "To maintain the structure of the content images and characteristics of the style images, Huang and Belongie introduce content loss and style loss, content loss ensures that the content of the generated image remains similar to that of the input content image by comparing their encoded features. And style loss ensures that the style of the generated image matches that of the input style image by comparing the statistics (mean and variance) of their encoded features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) SANet\n",
    "\n",
    "Park and Lee propose [SANet](https://arxiv.org/abs/1812.02342v5), an image style transfer architecture which aims to improve performance compared to previous work using a novel style-attentional network and identity loss function. \n",
    "\n",
    "Initially, the content and style images are forwarded to two pretrained VGG-19 encoders. To combine the global and local style patterns, two style-attentional networks (SANets) learns the mapping between the content and style features. The proposed SANet is a modified version of the [self-attention mechanism](https://arxiv.org/abs/1706.03762) proposed by Vaswani et al. The self-attention mechanism captures the relative dependencies within an input image by attending to each position and calculating their weighted importance. The SANets takes the encoded VGG content and style feature maps and creates a mapping of the content and style images. The SANet layer also upsamples its intermediate style map output which improves the model's ability to learn local style patterns. Finally, a 3x3 convolution combines the feature maps. The stylised image is then synthesised by the decoder.\n",
    "\n",
    "To maintain the structure of the content images and characteristics of the style images, Park and Lee propose an a novel identity loss function which unlike content and style losses, takes two of the same content or style images through the forward pass of the network. Thus, the identity loss evaluates how much the model deviates from the original characteristics of the content and style images.\n",
    "\n",
    "| ![sanet_architecture.png](./images/sanet_architecture.png) | \n",
    "|:--:| \n",
    "| *SANet Architecture* |\n",
    "\n",
    "Code adapted from https://github.com/GlebSBrykin/SANET/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Generative Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) MSG-Net\n",
    "\n",
    "Zhang and Dana propose [MSG-Net](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Zhang_Multi-style_Generative_Network_for_Real-time_Transfer_ECCVW_2018_paper.pdf), an image style transfer architecture which aims to improve on the limitations in flexibility of standard generative methods by using a Generative Network with novel CoMatch Layer and Upsample Convolution.\n",
    "\n",
    "The MSG-Net is composed of a Siamese network which inputs the content and style images and shares weights with the encoder of the Transformer network. In the feed-forward pass, the CoMatch Layer embeds the style images with a 2D representation and learns to match the second-order feature statistics (Gram Matrix) of the style targets during training. This enables multi-style generation from a single feed-forward network. At the Transformation network's bottleneck, its architecture has been extended to use an Upsampling Convolution operation which improves image quality by removing checkerboard artifacts. This operation applies an integer stride convolution and outputs an upsampled featuremap. \n",
    "\n",
    "The loss network adopts the VGG architecture to minimise the mean-squared error of the content and style losses, reducing the perceptual difference between the input and output images of the generative network learning. \n",
    "\n",
    "| ![msgnet.png](./images/msgnet_architecture.png) | \n",
    "|:--:| \n",
    "| *MSG-Net Architecture* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.CenterCrop(size=64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "content_trainloader, content_validloader, _, _, style_validloader, _ = data.get_dataloaders(bs=64, train_tf=train_tf, valid_tf=val_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "style_model = MSGNet.Net(ngf=128)\n",
    "optimizer = optim.Adam(params=style_model.parameters(), lr=1e-3)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Load pretrained vgg16\n",
    "vgg = MSGNet.Vgg16()\n",
    "\n",
    "def init_vgg16(model_folder='./models/output/MSG-Net/'):\n",
    "    if not os.path.exists(os.path.join(model_folder, 'vgg16.weight')):\n",
    "        if not os.path.exists(os.path.join(model_folder, 'vgg16.pth')):\n",
    "            assert ValueError(\"Pretrained weights must be prepared, download vgg16.t7 weights and convert to vgg16.pth first\")\n",
    "        \n",
    "        pretrained = torch.load('./models/output/MSG-Net/vgg16.pth')\n",
    "        for (src, dst) in zip(list(pretrained.items()), vgg.parameters()):\n",
    "            dst.data[:] = src[1]\n",
    "        torch.save(vgg.state_dict(), os.path.join('./models/output/MSG-Net/', 'vgg16_msg.pth'))\n",
    "        \n",
    "init_vgg16()\n",
    "\n",
    "vgg.load_state_dict(torch.load('./models/output/MSG-Net/vgg16_msg.pth'))\n",
    "\n",
    "style_loader = data.StyleLoader()\n",
    "\n",
    "# Helper functions\n",
    "def preprocess_batch(batch):\n",
    "    batch = batch.transpose(0, 1)\n",
    "    (r, g, b) = torch.chunk(batch, 3)\n",
    "    batch = torch.cat((b, g, r))\n",
    "    batch = batch.transpose(0, 1)\n",
    "    return batch\n",
    "\n",
    "def subtract_imagenet_mean_batch(batch):\n",
    "    \"\"\"Subtract ImageNet mean pixel-wise from a BGR image.\"\"\"\n",
    "    tensortype = type(batch.data)\n",
    "    mean = tensortype(batch.data.size())\n",
    "    mean[:, 0, :, :] = 103.939\n",
    "    mean[:, 1, :, :] = 116.779\n",
    "    mean[:, 2, :, :] = 123.680\n",
    "    return batch - Variable(mean.to(device))\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.earlystopper import EarlyStopper\n",
    "\n",
    "def msgnet_train(style_model, optimizer, mse_loss, vgg, content_loader, style_loader, num_epochs=10, batch_size=5, save_model_dir='./models/output/MSG-Net/'):\n",
    "    style_model.to(device)\n",
    "    vgg.to(device)\n",
    "    early_stopper = EarlyStopper()\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    total_losses = []\n",
    "    for e in range(num_epochs):\n",
    "        style_model.train()\n",
    "        agg_content_loss = 0.\n",
    "        agg_style_loss = 0.\n",
    "        count = 0\n",
    "        print('.' * 64)\n",
    "        print(f\"--- Epoch {e + 1}/{num_epochs} ---\")\n",
    "        pbar = tqdm(content_loader, leave=False)\n",
    "        for batch_id, x in enumerate(pbar):\n",
    "            pbar.set_description(f\"Epoch [{e + 1}/{num_epochs}]\")\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "            x = Variable(preprocess_batch(x))\n",
    "            x = x.to(device)\n",
    "\n",
    "            style_v = style_loader.get(batch_id)\n",
    "            style_model.setTarget(style_v)\n",
    "        \n",
    "            style_v = subtract_imagenet_mean_batch(style_v)\n",
    "            features_style = vgg(style_v)\n",
    "            \n",
    "            gram_style = [gram_matrix(y) for y in features_style]\n",
    "\n",
    "            y = style_model(x)\n",
    "            xc = Variable(x.data.clone())\n",
    "\n",
    "            y = subtract_imagenet_mean_batch(y)\n",
    "            xc = subtract_imagenet_mean_batch(xc)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_xc = vgg(xc)\n",
    "\n",
    "            f_xc_c = Variable(features_xc[1].data, requires_grad=False)\n",
    "            content_loss = 1.0 * mse_loss(features_y[1], f_xc_c)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for m in range(len(features_y)):\n",
    "                gram_y = gram_matrix(features_y[m])\n",
    "                gram_s = Variable(gram_style[m].data, requires_grad=False).repeat(batch_size, 1, 1, 1)\n",
    "                style_loss += 5.0 * mse_loss(gram_y.unsqueeze(1), gram_s[:n_batch, :, :])\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            agg_content_loss += content_loss.item()\n",
    "            agg_style_loss += style_loss.item()\n",
    "            \n",
    "            if (batch_id + 1) % (4 * 500) == 0:\n",
    "                # Save model\n",
    "                style_model.eval()\n",
    "                style_model.cpu()\n",
    "                save_model_filename = \"Epoch_\" + str(e) + \"iters_\" + str(count) + \"_\" + \\\n",
    "                    str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "                    1.0) + \"_\" + str(5.0) + \".model\"\n",
    "                save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
    "                torch.save(style_model.state_dict(), save_model_path)\n",
    "                style_model.train()\n",
    "                style_model.cuda()\n",
    "                pbar.set_description(\"\\nCheckpoint, trained model saved at\", save_model_path)\n",
    "        \n",
    "        mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "            time.ctime(), e + 1, count, len(content_loader),\n",
    "            agg_content_loss / (batch_id + 1),\n",
    "            agg_style_loss / (batch_id + 1),\n",
    "            (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
    "        )\n",
    "        print(mesg)\n",
    "        \n",
    "        content_losses.append(agg_content_loss / (batch_id + 1))\n",
    "        style_losses.append(agg_style_loss / (batch_id + 1))\n",
    "        agg_total_loss = agg_content_loss + agg_style_loss\n",
    "        total_losses.append(agg_total_loss / (batch_id + 1))\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopper.early_stop(total_loss):\n",
    "            print(f'Stopping early at Epoch {e + 1}, min val loss failed to decrease after {early_stopper.get_patience()} epochs')\n",
    "            break\n",
    "    \n",
    "    # Save model\n",
    "    style_model.eval()\n",
    "    style_model.cpu()\n",
    "    save_model_filename = \"Final_epoch_\" + str(num_epochs) + \"_\" + \\\n",
    "        str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "        1.0) + \"_\" + str(5.0) + \".model\"\n",
    "    save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
    "    torch.save(style_model.state_dict(), save_model_path)\n",
    "\n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
    "    return content_losses, style_losses, total_losses\n",
    "\n",
    "content_losses, style_losses, total_losses = msgnet_train(style_model, optimizer, mse_loss, vgg, content_trainloader, style_loader, num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(content_losses, style_losses, total_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chkpt model for evaluation\n",
    "style_model = MSGNet.Net(ngf=128)\n",
    "model_dict = torch.load('./models/output/MSG-Net/train_3.model')\n",
    "model_dict_clone = model_dict.copy()\n",
    "for key, value in model_dict_clone.items():\n",
    "    if key.endswith(('running_mean', 'running_var')):\n",
    "        del model_dict[key]\n",
    "style_model.load_state_dict(model_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def tensor_to_img(tensor, cuda=False):\n",
    "    (b, g, r) = torch.chunk(tensor, 3)\n",
    "    tensor = torch.cat((r, g, b))\n",
    "    if cuda:\n",
    "        img = tensor.clone().cpu().clamp(0, 255).numpy()\n",
    "    else:\n",
    "        img = tensor.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype('uint8')\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def msgnet_eval(style_model, content_loader, style_loader):\n",
    "    style_model.to(device)\n",
    "    ssim_sum = 0.0\n",
    "    fid_sum = 0.0\n",
    "    running_content_loss, running_style_loss = 0.0, 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for content, style in zip(content_loader, style_loader):\n",
    "            # Move content and style batches to device\n",
    "            content_images = content\n",
    "            content_images = content_images.to(device)\n",
    "            \n",
    "            style_images, style_labels = style\n",
    "            style_images, style_labels = style_images.to(device), style_labels.to(device)\n",
    "            \n",
    "            # Create stylised images\n",
    "            style_v = Variable(style_images)\n",
    "\n",
    "            content_images = Variable(content_images)\n",
    "            style_model.setTarget(style_v)\n",
    "            \n",
    "            output = style_model(content_images)\n",
    "            \n",
    "            # Compute quantitative evaluation metrics\n",
    "            ssim_sum += compute_ssim(content_images, output)\n",
    "            fid_sum += calculate_fid_from_dataset(content_images, output, device, dims=2048)\n",
    "            running_content_loss += calc_content_loss(content_images, output)\n",
    "            running_style_loss += calc_style_loss(style_images, output)\n",
    "            \n",
    "            # Convert output to PIL Image\n",
    "            stylised_images = []\n",
    "            for img in output:\n",
    "                img = tensor_to_img(img, cuda=True)\n",
    "                stylised_images.append(img)\n",
    "\n",
    "            # Display qualitative evaluation metrics on first batch\n",
    "            if total_samples == 0:\n",
    "                plot_results(content_images, style_images, style_labels, stylised_images, nrows=5, model_name=\"MSG-Net\", msgnet=True)\n",
    "\n",
    "            total_samples += style_labels.size(0)\n",
    "            \n",
    "    avg_ssim = ssim_sum / total_samples\n",
    "    avg_fid = fid_sum / total_samples\n",
    "    avg_content_loss = running_content_loss / total_samples\n",
    "    avg_style_loss = running_style_loss / total_samples\n",
    "    return avg_ssim, avg_fid, avg_content_loss, avg_style_loss\n",
    "\n",
    "msgnet_ssim, msgnet_fid, msgnet_content_loss, msgnet_style_loss = msgnet_eval(style_model, content_validloader, style_validloader)\n",
    "print(\"--- MSG-Net results ---\")\n",
    "print(f\"Average SSIM = {msgnet_ssim:.7f}\")\n",
    "print(f\"Average FID = {msgnet_fid:.4f}\")\n",
    "print(f\"Average content loss = {msgnet_content_loss:.4f}\")\n",
    "print(f\"Average style loss = {msgnet_style_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Hyper-parameters\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 25\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 100\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10.0\n",
    "lambda_id = 5.0\n",
    "\n",
    "# Dataset\n",
    "num_channels = 3\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "input_shape = (num_channels, img_height, img_width)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean and std of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std of dataset\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = cyclegan.ImageDataset(transform=test_tf, unaligned=True)\n",
    "\n",
    "total_mean_A, total_mean_B, total_std_A, total_std_B = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "for i in tqdm(range(0, len(dataset))):\n",
    "    batch = dataset[i]\n",
    "    img_A = batch['A']\n",
    "    img_B = batch['B']\n",
    "    \n",
    "    mean_A = torch.mean(img_A, dim=(1, 2))\n",
    "    mean_B = torch.mean(img_B, dim=(1, 2))\n",
    "    std_A = torch.std(img_A, dim=(1, 2))\n",
    "    std_B = torch.std(img_B, dim=(1, 2))\n",
    "    total_mean_A += mean_A\n",
    "    total_mean_B += mean_B\n",
    "    total_std_A += std_A\n",
    "    total_std_B += std_B\n",
    "    \n",
    "mean = (total_mean_A + total_mean_B) / (len(dataset) * 2)\n",
    "std = (total_std_A + total_std_B) / (len(dataset) * 2)\n",
    "print(mean) # tensor([0.4820, 0.4424, 0.3893]) \n",
    "print(std) # tensor([0.2064, 0.1953, 0.1880])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations and DataLoaders\n",
    "\n",
    "The ImageDataset is a paired dataset of random content and style image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(size=(img_width, img_height)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4820, 0.4424, 0.3893), (0.2064, 0.1953, 0.1880)),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(img_width, img_height)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Training data loader\n",
    "train_dataloader = DataLoader(\n",
    "    cyclegan.ImageDataset(transform=train_tf, unaligned=True),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    cyclegan.ImageDataset(transform=val_tf, unaligned=True, mode=\"val\"),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = cyclegan.GeneratorResNet(input_shape, n_residual_blocks)\n",
    "G_BA = cyclegan.GeneratorResNet(input_shape, n_residual_blocks)\n",
    "D_A = cyclegan.Discriminator(input_shape)\n",
    "D_B = cyclegan.Discriminator(input_shape)\n",
    "\n",
    "G_AB = G_AB.to(device)\n",
    "G_BA = G_BA.to(device)\n",
    "D_A = D_A.to(device)\n",
    "D_B = D_B.to(device)\n",
    "criterion_GAN.to(device)\n",
    "criterion_cycle.to(device)\n",
    "criterion_identity.to(device)\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "G_AB.apply(cyclegan.weights_init_normal)\n",
    "G_BA.apply(cyclegan.weights_init_normal)\n",
    "D_A.apply(cyclegan.weights_init_normal)\n",
    "D_B.apply(cyclegan.weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=cyclegan.LambdaLR(num_epochs, 0, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=cyclegan.LambdaLR(num_epochs, 0, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=cyclegan.LambdaLR(num_epochs, 0, decay_epoch).step\n",
    ")\n",
    "\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = cyclegan.ReplayBuffer()\n",
    "fake_B_buffer = cyclegan.ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, f\"./models/output/CycleGAN/images/{batches_done}.png\", normalize=False)\n",
    "\n",
    "# ----------\n",
    "#  Training function\n",
    "# ----------\n",
    "def cyclegan_train():\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    adv_losses = []\n",
    "    cycle_losses = []\n",
    "    I_losses = []\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('.' * 64)\n",
    "        print(f\"--- Epoch {epoch + 1}/{num_epochs} ---\")\n",
    "        pbar = tqdm(train_dataloader, leave=False)\n",
    "        agg_D_loss = 0.0\n",
    "        agg_G_loss = 0.0\n",
    "        agg_adv_loss = 0.0\n",
    "        agg_cycle_loss = 0.0\n",
    "        agg_I_loss = 0.0\n",
    "        for i, batch in enumerate(pbar):\n",
    "            pbar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "            agg_I_loss += loss_identity.item()\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "            agg_adv_loss += loss_GAN.item()\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "            agg_cycle_loss += loss_cycle.item()\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "            agg_G_loss += loss_G.item()\n",
    "            \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "            agg_D_loss += loss_D.item()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_dataloader) + i\n",
    "            batches_left = num_epochs * len(train_dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done)\n",
    "                \n",
    "\n",
    "        D_losses.append(agg_D_loss / (i + 1))\n",
    "        G_losses.append(agg_G_loss / (i + 1))\n",
    "        adv_losses.append(agg_adv_loss / (i + 1))\n",
    "        cycle_losses.append(agg_cycle_loss / (i + 1))\n",
    "        I_losses.append(agg_I_loss / (i + 1))\n",
    "        \n",
    "        # Print log\n",
    "        print(f\"[D loss: {loss_D.item()}] [G loss: {loss_G.item()}, adv: {loss_GAN.item()}, cycle: {loss_cycle.item()}, identity: {loss_identity.item()}] ETA: {time_left}\")\n",
    "\n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "\n",
    "    torch.save(G_AB.state_dict(), f\"./models/output/CycleGAN/G_AB_trained_{epoch}.pth\")\n",
    "    torch.save(G_BA.state_dict(), f\"./models/output/CycleGAN/G_BA_trained_{epoch}.pth\")\n",
    "    torch.save(D_A.state_dict(), f\"./models/output/CycleGAN/D_A_trained_{epoch}.pth\")\n",
    "    torch.save(D_B.state_dict(), f\"./models/output/CycleGAN/D_B_trained_{epoch}.pth\")\n",
    "            \n",
    "    return D_losses, G_losses, adv_losses, cycle_losses, I_losses\n",
    "\n",
    "D_losses, G_losses, adv_losses, cycle_losses, I_losses = cyclegan_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses)\n",
    "plt.plot(G_losses)\n",
    "plt.plot(adv_losses)\n",
    "plt.plot(cycle_losses)\n",
    "plt.plot(I_losses)\n",
    "plt.title('Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discriminator', 'Generator', 'GAN', 'Cycle', 'Identity'], loc='upper left')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses)\n",
    "plt.plot(adv_losses)\n",
    "plt.plot(cycle_losses)\n",
    "plt.plot(I_losses)\n",
    "plt.title('Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discriminator', 'GAN', 'Cycle', 'Identity'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model for style transfer\n",
    "G_AB = cyclegan.GeneratorResNet(input_shape, n_residual_blocks)\n",
    "G_AB.load_state_dict(torch.load(\"./models/output/CycleGAN/G_AB_trained_99.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    cyclegan.ImageDatasetTesting(transform=test_tf, mode=\"val\"),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def cyclegan_eval(G_AB, test_dataloader):\n",
    "    G_AB.to(device)\n",
    "    ssim_sum = 0.0\n",
    "    fid_sum = 0.0\n",
    "    running_content_loss, running_style_loss = 0.0, 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_dataloader)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "            style_labels = batch[\"label\"]\n",
    "            output = G_AB(real_A)\n",
    "            \n",
    "            # Compute quantitative evaluation metrics\n",
    "            ssim_sum += compute_ssim(real_A, output)\n",
    "            fid_sum += calculate_fid_from_dataset(real_A, output, device, dims=2048)\n",
    "            running_content_loss += calc_content_loss(real_A, output)\n",
    "            running_style_loss += calc_style_loss(real_B, output)\n",
    "\n",
    "            # Display qualitative evaluation metrics on first batch\n",
    "            if total_samples == 0:\n",
    "                plot_results(real_A, real_B, style_labels, output, nrows=5, model_name=\"CycleGAN\")\n",
    "            total_samples += real_A.size(0)\n",
    "            \n",
    "    avg_ssim = ssim_sum / total_samples\n",
    "    avg_fid = fid_sum / total_samples\n",
    "    avg_content_loss = running_content_loss / total_samples\n",
    "    avg_style_loss = running_style_loss / total_samples\n",
    "    return avg_ssim, avg_fid, avg_content_loss, avg_style_loss\n",
    "\n",
    "cyclegan_ssim, cyclegan_fid, cyclegan_content_loss, cyclegan_style_loss = cyclegan_eval(G_AB, test_dataloader)\n",
    "\n",
    "print(\"--- CycleGAN results ---\")\n",
    "print(f\"Average SSIM = {cyclegan_ssim:.7f}\")\n",
    "print(f\"Average FID = {cyclegan_fid:.4f}\")\n",
    "print(f\"Average content loss = {cyclegan_content_loss:.4f}\")\n",
    "print(f\"Average style loss = {cyclegan_style_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id=\"results\">Results</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id=\"discussion\">Discussion</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
